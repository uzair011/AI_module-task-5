{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBY+GFjcQGEAGUMj4iAq0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uzair011/AI_module-task-5/blob/main/CNN_week7_2598219.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mohamed Uzair Liyauddeen\n",
        "# 2598219\n",
        "# comments added as an explanation insted of the report. thank you.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Data Loading and Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),          # Convert images to tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values to [-1, 1]\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 2. Define the CNN Architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)    # 1 input, 6 outputs, 5x5 kernel\n",
        "        self.pool = nn.MaxPool2d(2, 2)     # 2x2 pooling\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)   # 6 inputs, 16 outputs\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Conv -> ReLU -> Pool\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)              # Flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)                      # Output logits\n",
        "        return x\n",
        "\n",
        "# 3. Initialize Model, Loss, Optimizer\n",
        "net = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 4. Training Loop (10 epochs)\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:  # Print every 200 mini-batches\n",
        "            print(f'Epoch {epoch+1}, Batch {i+1}: Loss {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0\n",
        "print('Training Complete')\n",
        "\n",
        "# 5. Evaluation\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0qNyx8Yg0p8",
        "outputId": "0c1f0d8c-2134-481b-f1a4-6dcb3350478c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 348kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.21MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.38MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 200: Loss 2.300\n",
            "Epoch 1, Batch 400: Loss 2.280\n",
            "Epoch 1, Batch 600: Loss 2.208\n",
            "Epoch 1, Batch 800: Loss 1.575\n",
            "Epoch 2, Batch 200: Loss 0.506\n",
            "Epoch 2, Batch 400: Loss 0.388\n",
            "Epoch 2, Batch 600: Loss 0.329\n",
            "Epoch 2, Batch 800: Loss 0.273\n",
            "Epoch 3, Batch 200: Loss 0.226\n",
            "Epoch 3, Batch 400: Loss 0.206\n",
            "Epoch 3, Batch 600: Loss 0.177\n",
            "Epoch 3, Batch 800: Loss 0.170\n",
            "Epoch 4, Batch 200: Loss 0.147\n",
            "Epoch 4, Batch 400: Loss 0.141\n",
            "Epoch 4, Batch 600: Loss 0.136\n",
            "Epoch 4, Batch 800: Loss 0.131\n",
            "Epoch 5, Batch 200: Loss 0.112\n",
            "Epoch 5, Batch 400: Loss 0.121\n",
            "Epoch 5, Batch 600: Loss 0.107\n",
            "Epoch 5, Batch 800: Loss 0.110\n",
            "Epoch 6, Batch 200: Loss 0.090\n",
            "Epoch 6, Batch 400: Loss 0.102\n",
            "Epoch 6, Batch 600: Loss 0.091\n",
            "Epoch 6, Batch 800: Loss 0.095\n",
            "Epoch 7, Batch 200: Loss 0.086\n",
            "Epoch 7, Batch 400: Loss 0.087\n",
            "Epoch 7, Batch 600: Loss 0.084\n",
            "Epoch 7, Batch 800: Loss 0.081\n",
            "Epoch 8, Batch 200: Loss 0.074\n",
            "Epoch 8, Batch 400: Loss 0.074\n",
            "Epoch 8, Batch 600: Loss 0.078\n",
            "Epoch 8, Batch 800: Loss 0.074\n",
            "Epoch 9, Batch 200: Loss 0.067\n",
            "Epoch 9, Batch 400: Loss 0.073\n",
            "Epoch 9, Batch 600: Loss 0.068\n",
            "Epoch 9, Batch 800: Loss 0.067\n",
            "Epoch 10, Batch 200: Loss 0.061\n",
            "Epoch 10, Batch 400: Loss 0.063\n",
            "Epoch 10, Batch 600: Loss 0.061\n",
            "Epoch 10, Batch 800: Loss 0.068\n",
            "Training Complete\n",
            "Test Accuracy: 98.31%\n"
          ]
        }
      ]
    }
  ]
}